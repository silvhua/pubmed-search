{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append(r\"/home/silvhua/custom_python\")\n",
    "from silvhua import *\n",
    "from Pubmed_API import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the option to wrap text within cells\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('api_ncbi') # Pubmed API key\n",
    "result_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:39:43,597 - Pubmed_API - INFO:\n",
      "Search term: exercise\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:39:49,243 - Pubmed_API - INFO:\n",
      "Extracting these 5 PMIDs: ['38581603', '38581560', '38581554', '38581479', '38581449']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:40:17,053 - Pubmed_API - INFO:\n",
      "Processing complete.\n",
      "\n",
      "2024-04-06 20:40:17,079 - Pubmed_API - INFO:\n",
      "5 PMIDs found.\n",
      "['38581603', '38581560', '38581554', '38581479', '38581449']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Pubmed_API:\n",
    "    def __init__(self, api_key=os.getenv('api_ncbi'), logger=None, logging_level=logging.INFO):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - api_key (str): NCBI API key\n",
    "        ---\n",
    "        # Example usage\n",
    "\n",
    "        result_dict = dict()\n",
    "        iteration = 1\n",
    "        query = 'query string'\n",
    "        result_dict[iteration] = Pubmed_API()\n",
    "\n",
    "        ## Option 1\n",
    "\n",
    "        2 steps: Get list of PMIDs first, then get the article data.\n",
    "\n",
    "        ids_list = result_dict[iteration].search_article(query, retmax=5, ids_only=True)\n",
    "        df = result_dict[iteration].get_article_data_by_title()\n",
    "\n",
    "        ## Option 2\n",
    "\n",
    "        Get the PMIDs and then the article data in one step.\n",
    "\n",
    "        df = result_dict[iteration].search_article(query, retmax=5, ids_only=False)\n",
    "\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.logger = create_function_logger('Pubmed_API', logger, level=logging_level)\n",
    "        self.iteration = 0\n",
    "        self.responses_dict = {}\n",
    "        self.results_dict = {}\n",
    "        self.PMIDs_dict = {}\n",
    "        self.record_strings_dict = {}\n",
    "\n",
    "    def search_article(self, query, query_tag=None, publication=None, reldate=None, retmax=None,\n",
    "        systematic_only=False, review_only=False, period_filter=None,\n",
    "        additional_search_params=None, ids_only=False, \n",
    "        verbose=True\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Search for article title in PubMed database.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): Pubmed search query.\n",
    "        - reldate (int, optional): The search returns only those items that have a date specified by datetype within the last n days.\n",
    "        - query_tag (str, optional): Query tag to append to the search query.\n",
    "        - publication (str, optional): Publication name.\n",
    "        - retmax (int, optional): Maximum number of results to return. \n",
    "            If None, default is 20. API returns a maximum of 9999 results. To get more results for Pubmed,\n",
    "            need to use the command line: https://www.ncbi.nlm.nih.gov/books/NBK179288/\n",
    "        - systematic_only (bool, optional): If True, filter for only systematic review articles.\n",
    "        - review_only (bool, optional): If True, filter for only systematic review or review articles.\n",
    "        - additional_search_params (dict, optional): Additional search parameters to pass to the esearch API.\n",
    "        - period_filter (1, 5, or 10, optional): Filter for articles published in the past 1, 5, or 10 years.\n",
    "            Note: To filter by other periods, use `reldate` parameter as that is how the API works.\n",
    "            \n",
    "        Returns:\n",
    "\n",
    "\n",
    "        API documentation: https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "        Pubmed User Guide including tags for filtering results: https://pubmed.ncbi.nlm.nih.gov/help/\n",
    "        \"\"\"\n",
    "        base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "        if self.api_key:\n",
    "            base_url += f'&api_key={self.api_key}'\n",
    "        response = {}\n",
    "        results = pd.DataFrame()\n",
    "        search_term = f'{re.sub(r\"not\", \"\", query)}'  # Remove 'not' since it will be treated as a boolean\n",
    "        if query_tag:\n",
    "            search_term += f'{query_tag}'\n",
    "        if publication:\n",
    "            search_term = f'AND {publication} [ta]'\n",
    "        if systematic_only:\n",
    "            search_term += ' AND systematic[sb]'\n",
    "        elif review_only:\n",
    "            search_term += ' AND (systematic[sb] OR review[pt])'\n",
    "        if period_filter:\n",
    "            search_term += f' AND y_{period_filter}[Filter]'\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': search_term,\n",
    "            'retmode': 'json',\n",
    "            'datetype': 'edat',\n",
    "        }\n",
    "        if reldate:\n",
    "            params['reldate'] = reldate\n",
    "        if retmax:\n",
    "            params['retmax'] = retmax\n",
    "        if additional_search_params:\n",
    "            params.update(additional_search_params)\n",
    "        self.logger.info(f'Search term: {search_term}')\n",
    "        messages = []\n",
    "        try:\n",
    "            self.iteration += 1\n",
    "            response = requests.get(base_url, params=params)\n",
    "            response_dict = response.json()\n",
    "            id_list = response_dict['esearchresult']['idlist']\n",
    "            messages.append(f'{len(id_list)} PMIDs found.')\n",
    "            if verbose==True:\n",
    "                messages.append(f'{id_list}')\n",
    "            self.PMIDs_dict[self.iteration] = id_list\n",
    "            self.responses_dict[self.iteration] = response_dict\n",
    "            if ids_only==False:\n",
    "                results = self.get_article_data_by_title()\n",
    "            else:\n",
    "                results = id_list\n",
    "            self.logger.info('\\n'.join(messages))\n",
    "        except Exception as error:\n",
    "            error_messages = []\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            file = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = file.f_code.co_filename\n",
    "            message = f'\\tAn error occurred on line {lineno} in {filename}: {error}'\n",
    "            error_messages.append(message)\n",
    "            self.logger.error('\\n'.join(error_messages))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_article_data_by_title(self, iteration=None):\n",
    "        result_df = pd.DataFrame()\n",
    "        try:\n",
    "            result_dict = {}\n",
    "            iteration = self.iteration if iteration == None else iteration\n",
    "            record_strings_list = self.batch_retrieve_citation(iteration)\n",
    "            self.record_strings_dict[iteration] = record_strings_list\n",
    "            for index, record_string in enumerate(record_strings_list):\n",
    "                result_dict[index] = self.extract_pubmed_details(record_string)\n",
    "            self.results_dict[iteration] = result_dict\n",
    "            result_df = pd.DataFrame(result_dict).transpose()\n",
    "        except Exception as error:\n",
    "            error_messages = []\n",
    "            error_messages.append(f'Response: \\n{self.PMIDs_dict.get(iteration)}')\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            file = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = file.f_code.co_filename\n",
    "            message = f'\\tAn error occurred on line {lineno} in {filename}: {error}'\n",
    "            error_messages.append(message)\n",
    "            self.logger.error('\\n'.join(error_messages))\n",
    "        return result_df\n",
    "\n",
    "    def batch_retrieve_citation(self, iteration):\n",
    "        result_list = []\n",
    "        messages = []\n",
    "        try:\n",
    "            id_list = self.PMIDs_dict.get(iteration)\n",
    "            if id_list:\n",
    "                self.logger.info(f'Extracting these {len(id_list)} PMIDs: {id_list}')\n",
    "                for index, id in enumerate(id_list):\n",
    "                    result_list.append(self.retrieve_citation(id).decode('utf-8'))\n",
    "                    current_index, current_id = index+1, id\n",
    "                    # Show progress \n",
    "                    print('.', end='\\n' if current_index%2==0 else '')\n",
    "                self.logger.info(\"Processing complete.\")\n",
    "            else:\n",
    "                self.logger.warning(f'No results found.')\n",
    "        except Exception as error:\n",
    "            messages.append(f'Response: \\n{self.responses_dict.get(iteration)}')\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            file = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = file.f_code.co_filename\n",
    "            messages.append(f'\\tAn error occurred on line {lineno} in {filename}: {error}')\n",
    "            messages.append(f'Article {current_index} [{current_id}] not found.')\n",
    "        return result_list\n",
    "\n",
    "    def retrieve_citation(self, article_id):\n",
    "        base_url = f'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "        if self.api_key:\n",
    "            base_url += f'&api_key={self.api_key}'\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'id': article_id\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        return response.content\n",
    "\n",
    "    def extract_pubmed_details_df(self, iteration=None):\n",
    "        \"\"\"\n",
    "        Extract the Pubmed article details for the given list of record strings for the given iteration.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame of the Pubmed article details.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame()\n",
    "        record_strings = pd.Series(self.record_strings_dict.get(iteration if iteration else self.iteration))\n",
    "        regex_dict = {\n",
    "            'article_title': r'<ArticleTitle>(.*?)</ArticleTitle>',\n",
    "            'pmid': r'<PMID.*?>(.*?)</PMID>',\n",
    "            'journal': r'<Title>(.*?)</Title>',\n",
    "            'volume': r'<Volume>(.*?)</Volume>',\n",
    "            'issue': r'<Issue>(.*?)</Issue>',\n",
    "            'year': r'<PubDate><Year>(\\d{4})</Year>',\n",
    "            'month': r'<PubDate>.*?<Month>(Aug)</Month>.*?</PubDate>',\n",
    "            'start_page': r'<StartPage>(.*?)</StartPage>',\n",
    "            'end_page': r'<EndPage>(.*?)</EndPage>',\n",
    "            'doi': r'<ELocationID.*?EIdType=\"doi\".*?>(.*?)</ELocationID>',\n",
    "        }\n",
    "        for column, regex in regex_dict.items():\n",
    "            df[column] = record_strings.str.extract(regex)\n",
    "        df['abstract'] = self.df_extractall(\n",
    "            record_strings, parent_regex=r'<Abstract>(.*?)</Abstract>',\n",
    "            regex = r'<AbstractText.*?(?: Label=\"(.*?)\")?.*?>(.*?)</AbstractText>',\n",
    "            logger=self.logger, sep=': ', join_strings=' '\n",
    "        )\n",
    "        df['mesh_headings'] = self.df_extractall(\n",
    "            record_strings, \n",
    "            parent_regex=r'<MeshHeadingList>(.*?)</MeshHeadingList>',\n",
    "            regex=r'<MeshHeading><DescriptorName.*?>(.*?)</DescriptorName>(<QualifierName.*?>.*?</QualifierName>)?</MeshHeading>',\n",
    "            nested_regex=r'<QualifierName.*?>(.*?)</QualifierName>', logger=self.logger\n",
    "        )\n",
    "        df['authors'] = self.df_extractall(\n",
    "            record_strings, sep=' ',\n",
    "            regex=r'<Author ValidYN=\"Y\".*?><LastName>(.*?)</LastName><ForeName>(.*?)</ForeName>',\n",
    "            logger=self.logger \n",
    "        )\n",
    "        df['keywords'] = self.df_extractall(\n",
    "            record_strings, parent_regex=r'<KeywordList.*?>(.*?)</KeywordList>',\n",
    "            regex=r'<Keyword.*?>(.*?)</Keyword>', \n",
    "            logger=self.logger\n",
    "        )\n",
    "        df['major_topics'] = self.df_extractall(\n",
    "            record_strings, \n",
    "            regex=r'<[^>]*MajorTopicYN=\"Y\"[^>]*>([^<]+)<\\/[^>]+>', \n",
    "            logger=self.logger\n",
    "        )\n",
    "        df['publication_type'] = self.df_extractall(\n",
    "            record_strings, parent_regex=r'<PublicationTypeList.*?>(.*?)</PublicationTypeList>',\n",
    "            regex=r'<PublicationType.*?>(.*?)</PublicationType>', \n",
    "            logger=self.logger\n",
    "        )\n",
    "        columns = [\n",
    "            'article_title',\n",
    "            'abstract',\n",
    "            'mesh_headings',\n",
    "            'keywords',\n",
    "            'major_topics',\n",
    "            'pmid',\n",
    "            'doi',\n",
    "            'journal',\n",
    "            'volume',\n",
    "            'issue',\n",
    "            'year',\n",
    "            'month',\n",
    "            'start_page',\n",
    "            'end_page',\n",
    "            'authors',\n",
    "            'publication_type'\n",
    "        ]\n",
    "        return df[columns]\n",
    "\n",
    "    def df_extractall(self, \n",
    "            series, regex, parent_regex=None, nested_regex=None, sep=[' ', ' / '], \n",
    "            join_strings=False, logger=None\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Helper function called by `.search_article()` and `.get_article_data_by_title()` to parse \n",
    "        article metadata from PubMed database.\n",
    "\n",
    "        Parameters:\n",
    "        - series: pd.Series\n",
    "        - regex: Regular expression to extract from the series.\n",
    "        - parent_regex (optional): Regular expression from which to extract the `regex`.\n",
    "            If None, `regex` will be extracted from the series.\n",
    "        - nested_regex (optional): Regular expression that is nested within `regex` to extract.\n",
    "        - sep (str or list; optional): String or list of strings used to separate multiple capture groups.\n",
    "            If it is a list, then the first value is used to separate the main capture groups. \n",
    "            The second value is used to separate the nested capture groups. If the nested regex \n",
    "            has multiple capture groups, then the last value is used to separate them.\n",
    "        - join_strings (optional): Boolean indicating whether to join the extracted values.\n",
    "        - logger (optional): Instance of Custom_Logger class.\n",
    "\n",
    "        Returns:\n",
    "        - pd.Series with the extracted values.\n",
    "        \"\"\"\n",
    "        logger = create_function_logger('df_extractall', logger)\n",
    "        messages = []\n",
    "        messages.append(f'***Running `df_extractall` with regex {regex}***')\n",
    "        if parent_regex:\n",
    "            messages.append(f'\\tparent_regex: {parent_regex}')\n",
    "        if nested_regex:\n",
    "            messages.append(f'\\tnested_regex: {nested_regex}')\n",
    "        if parent_regex:\n",
    "            extracted = series.str.extract(parent_regex, expand=False)\n",
    "            series = extracted\n",
    "        extracted = series.str.extractall(regex).replace({np.nan: ''})\n",
    "        if extracted.shape[1] >= 1:\n",
    "            joined_values = extracted[0]\n",
    "        else:\n",
    "            messages.warning('No matches found.')\n",
    "            return series\n",
    "        if extracted.shape[1] > 1:\n",
    "            extracted.index.names = [f'{name if name else \"index\"}{index if index !=0 else \"\"}' for index, name in enumerate(extracted.index.names)]\n",
    "            for i in range(1, extracted.shape[1]):\n",
    "                if nested_regex:\n",
    "                    matches = extracted[i].str.extractall(nested_regex)#.replace({np.nan: ''})\n",
    "                    messages.append(f'Number of nested capture groups: {matches.shape[1]}')\n",
    "                    matches.columns = [f'nested_text{column}' for column in matches.columns]\n",
    "                    regex_df = extracted.merge(\n",
    "                        matches, how='left', left_index=True, right_index=True\n",
    "                    ).replace({np.nan: ''})\n",
    "                    nested_separator = sep if type(sep) == str else sep[1]\n",
    "                    if i == 1:\n",
    "                        root_column = 0 \n",
    "                        capture_group_separator = nested_separator\n",
    "                    else:\n",
    "                        root_column = 'Text'\n",
    "                        capture_group_separator = sep if type(sep) == str else sep[-1]\n",
    "                    regex_df = concat_columns(\n",
    "                        regex_df, [root_column, 'nested_text0'], 'Text', \n",
    "                        sep=capture_group_separator\n",
    "                    )\n",
    "                    joined_values = regex_df['Text']\n",
    "\n",
    "                else:\n",
    "                    separator = sep if type(sep) == str else sep[0]\n",
    "                    joined_values = joined_values + separator + extracted[i]\n",
    "        new_series = joined_values.groupby(level=0).apply(lambda groupby: [match for match in groupby])\n",
    "        if (type(join_strings) == str) | (join_strings == True):\n",
    "            new_series = new_series.apply(lambda x: f'{join_strings if type(join_strings) == str else \" \"}'.join(x))\n",
    "        logger.debug('\\n'.join(messages))\n",
    "        return new_series\n",
    "\n",
    "    def extract_pubmed_details(self, record_string):\n",
    "        \"\"\"\n",
    "        [Archived: Use `extract_pubmed_details_df` instead to perform regex operations on the entire dataframe.]\n",
    "        Helper function called by `pubmed_details_by_title` to parse article metadata from PubMed database.\n",
    "        \"\"\"\n",
    "        authors = re.findall(r'<Author ValidYN=\"Y\".*?><LastName>(.*?)</LastName><ForeName>(.*?)</ForeName>', record_string)\n",
    "        formatted_authors = ', '.join(['{} {}'.format(author[1], author[0]) for author in authors])\n",
    "\n",
    "        # Extract publication year\n",
    "        publication_year = re.search(r'<PubDate><Year>(\\d{4})</Year>', record_string)\n",
    "        publication_year = publication_year.group(1) if publication_year else ''\n",
    "        publication_month = re.search(r'<PubDate>.*?<Month>(Aug)</Month>.*?</PubDate>', record_string)\n",
    "        publication_month = publication_month.group(1) if publication_month else ''\n",
    "\n",
    "        # Extract article title\n",
    "        article_title = re.search(r'<ArticleTitle>(.*?)</ArticleTitle>', record_string)\n",
    "        article_title = article_title.group(1) if article_title else ''\n",
    "\n",
    "        # Extract journal title\n",
    "        journal_title = re.search(r'<Title>(.*?)</Title>', record_string)\n",
    "        journal_title = journal_title.group(1) if journal_title else ''\n",
    "\n",
    "        # Extract journal volume\n",
    "        journal_volume = re.search(r'<Volume>(.*?)</Volume>', record_string)\n",
    "        journal_volume = journal_volume.group(1) if journal_volume else ''\n",
    "\n",
    "        # Extract journal issue\n",
    "        journal_issue = re.search(r'<Issue>(.*?)</Issue>', record_string)\n",
    "        journal_issue = journal_issue.group(1) if journal_issue else ''\n",
    "\n",
    "        # Extract start page\n",
    "        start_page = re.search(r'<StartPage>(.*?)</StartPage>', record_string)\n",
    "        start_page = start_page.group(1) if start_page else ''\n",
    "\n",
    "        # Extract end page\n",
    "        end_page = re.search(r'<EndPage>(.*?)</EndPage>', record_string)\n",
    "        end_page = end_page.group(1) if end_page else ''\n",
    "\n",
    "        # Extract ELocationID\n",
    "        doi = re.search(r'<ELocationID.*?EIdType=\"doi\".*?>(.*?)</ELocationID>', record_string)\n",
    "        doi = doi.group(1) if doi else ''\n",
    "\n",
    "        # Extract PMID\n",
    "        pmid = re.search(r'<PMID.*?>(.*?)</PMID>', record_string)\n",
    "        pmid = pmid.group(1) if pmid else ''\n",
    "\n",
    "        abstract_matches = re.findall(r'(<AbstractText.*?>.*?</AbstractText>)', record_string)\n",
    "        # self.logger.debug(f'Number of abstract sections: {len(abstract_matches)}')\n",
    "        if len(abstract_matches) > 1:\n",
    "            cleaned_abstract_sections = []\n",
    "            for match in abstract_matches:\n",
    "                clean_match = re.sub(r'<AbstractText.*?((?:Label=\".*\")?.*?>.*)</AbstractText>', r'\\1', match)\n",
    "                clean_match = re.sub(r'(?: Label=\"(.*?)\")?.*?>(.*)', r'\\1: \\2', clean_match)\n",
    "                cleaned_abstract_sections.append(clean_match)\n",
    "                \n",
    "            abstract = ''.join([f'{group}<br>' for group in cleaned_abstract_sections])\n",
    "        else:\n",
    "            abstract = re.sub(r'<AbstractText.*?>(.*?)</AbstractText>', r'\\1', abstract_matches[0])  if abstract_matches else ''\n",
    "            \n",
    "        # Extract MeshHeadingList\n",
    "        MeshHeadingList = re.search(r'<MeshHeadingList>(.*?)</MeshHeadingList>', record_string)\n",
    "        MeshHeadingList = MeshHeadingList.group(1) if MeshHeadingList else ''\n",
    "\n",
    "        # Estract MeshHeading text and any QualifierName\n",
    "        mesh_headings = []\n",
    "        pattern = r'<MeshHeading><DescriptorName.*?>(.*?)</DescriptorName>(<QualifierName.*?>.*?</QualifierName>)?</MeshHeading>'\n",
    "        matches = re.findall(pattern, MeshHeadingList)\n",
    "        for match in matches:\n",
    "            heading = match[0]\n",
    "            if match[1]: # Estract Mesh QualifierName                \n",
    "                MeshQualifiers = re.findall(\n",
    "                    r'<QualifierName.*?>(.*?)</QualifierName>', match[1]\n",
    "                    )\n",
    "                print(f'mesh qualifiers: {MeshQualifiers}')\n",
    "                for qualifier in MeshQualifiers:\n",
    "                    heading = f\"{match[0]} / {qualifier}\"\n",
    "                    mesh_headings.append(heading)\n",
    "            else:\n",
    "                mesh_headings.append(heading)\n",
    "\n",
    "        # Extract keyword\n",
    "        Keyword_List = re.search(r'<KeywordList.*?>(.*?)</KeywordList>', record_string)\n",
    "        Keyword_List = Keyword_List.group(1) if Keyword_List else ''\n",
    "        Keywords = re.findall(\n",
    "            r'<Keyword.*?>(.*?)</Keyword>', Keyword_List\n",
    "            )\n",
    "        # Extract MajorTopic text\n",
    "        MajorTopics = re.findall(\n",
    "            r'<[^>]*MajorTopicYN=\"Y\"[^>]*>([^<]+)<\\/[^>]+>', record_string\n",
    "            )\n",
    "        # Extract Publication Type\n",
    "        PublicationTypeList = re.search(r'<PublicationTypeList.*?>(.*?)</PublicationTypeList>', record_string)\n",
    "        PublicationTypeList = PublicationTypeList.group(1) if PublicationTypeList else ''\n",
    "        PublicationType = re.findall(\n",
    "            r'<PublicationType.*?>(.*?)</PublicationType>', PublicationTypeList\n",
    "            )\n",
    "        return {\n",
    "            'pubmed_title': article_title,\n",
    "            'abstract': abstract,\n",
    "            'journal': journal_title,\n",
    "            'authors': formatted_authors,\n",
    "            'year': publication_year,\n",
    "            'month': publication_month,\n",
    "            'pub_volume': journal_volume,\n",
    "            'pub_issue': journal_issue,\n",
    "            'start_page': start_page,\n",
    "            'end_page': end_page,\n",
    "            'doi': doi,\n",
    "            'pmid': pmid,\n",
    "            'mesh_headings': mesh_headings,\n",
    "            'keywords': Keywords,\n",
    "            'major_topics': MajorTopics,\n",
    "            'publication_type': PublicationType\n",
    "        }\n",
    "\n",
    "\n",
    "iteration = 1\n",
    "query = 'exercise'\n",
    "retmax = 5\n",
    "result_dict[iteration] = Pubmed_API()\n",
    "df = result_dict[iteration].search_article(query, retmax=retmax, ids_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubmed",
   "language": "python",
   "name": "pubmed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
